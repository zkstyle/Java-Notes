##redis为什么那么快
我先给个我的结论，单线程的Redis在瓶颈是cpu的io时（这不是大多数应用的实际应用场景），确实速度会比多线程慢。但是，我们实际应用场景中很少会遇到瓶颈是CPU的io的情况，这时候单线程优势就凸显出来了。

实现很简单！性能又不会比多线程差，并且，单线程确实不用处理上下文的切换，cpu利用率会比多线程高，这时候采用单线程实现是一种很划算的做法。当然，如果你的宽带和内存牛逼到了使得你的io成为瓶颈，这时候也只能使用多线程了。


先说说Redis是什么吧小老弟？

Redis嘛，就是一种运行速度很快，并发很强的跑在内存上的NoSql数据库，支持键到五种数据类型的映射。
###讲一讲为什么Redis这么快？

首先，采用了多路复用io阻塞机制
阻塞IO
如果接收缓冲区的数据为空的时候，那么receive调用scoket的read方法就会处于阻塞状态，直到有数据过来。
同样，对于写来说，如果发送缓冲区满了，那么调用scoket的write方法就会处于阻塞状态，直到报文送到网络上。
这就是阻塞IO！！

非阻塞IO
阻塞IO会一直等待，所以非阻塞IO是用来解决IO线程与scoket之间的解耦问题（引入事件机制），如果scoket发送缓存区可写的话会通知IO线程进行write，同样如果scoket的接受缓冲区可读的话会通知IO线程进行read。
这就是非阻塞IO！！

IO多路复用
而这个事件机制就是IO多路复用的模型，在linux可以使用select与epoll，可以产生多个线程去处理，如果线程去读的时候发现tcp缓冲区还没准备好的话，线程不会等待，会丢到select调度系统里面（调度吸引里面有n个这样的线程），如果tcp缓冲区准备好的就会发送事件通知select，从select选一个线程去执行。
进程调用select/poll/epoll，select/poll/epoll会将进程block起来；

kernel会‘监视’所有select负责的socket；

当任何一个socket准备好数据，select就会返回（也就是图中的return medable）；

进程调用read（图中第二次 system call），将数据从kernel拷贝到用户进程；

总结：所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。
I/O多路复用和阻塞I/O差别其实不大，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。

但是，I/O多路复用的优势是：**同时处理多个连接请求**。

所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用多线程 + 阻塞 IO的web server性能更好，可能延迟还更大。

select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。

然后，数据结构简单，操作节省时间
最后，运行在内存中，自然速度快
###Redis为什么是单线程的？

Redis官方很敷衍就随便给了一点解释。不过基本要点也都说了，因为Redis的瓶颈不是cpu的运行速度，而往往是网络带宽和机器的内存大小。再说了，单线程切换开销小，容易实现既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。
如果万一CPU成为你的Redis瓶颈了，或者，你就是不想让服务器其他核闲置，那怎么办？

那也很简单，你多起几个Redis进程就好了。Redis是key-value数据库，又不是关系数据库，数据之间没有约束。只要客户端分清哪些key放在哪个Redis进程上就可以了。redis-cluster可以帮你做的更好。
单线程可以处理高并发请求吗？

当然可以了，Redis都实现了。有一点概念需要澄清，并发并不是并行。
（相关概念：并发性I/O流，意味着能够让一个计算单元来处理来自多个客户端的流请求。并行性，意味着服务器能够同时执行几个事情，具有多个计算单元）
我们使用单线程的方式是无法发挥多核CPU 性能，有什么办法发挥多核CPU的性能嘛？

我们可以通过在单机开多个Redis 实例来完善！
警告：这里我们一直在强调的单线程，只是在处理我们的网络请求的时候只有一个线程来处理，一个正式的Redis Server运行的时候肯定是不止一个线程的，这里需要大家明确的注意一下！
例如Redis进行持久化的时候会以子进程或者子线程的方式执行（具体是子线程还是子进程待读者深入研究）
###简述一下Redis值的五种类型

String 整数，浮点数或者字符串
Set 集合
Zset 有序集合
Hash 散列表
List 列表




###有序集合的实现方式是哪种数据结构？

跳跃表。

###请列举几个用得到Redis的常用使用场景?
~~~
缓存，毫无疑问这是Redis当今最为人熟知的使用场景。再提升服务器性能方面非常有效；

排行榜，在使用传统的关系型数据库（mysql oracle 等）来做这个事儿，非常的麻烦，而利用Redis的SortSet(有序集合)数据结构能够简单的搞定；

计算器/限速器，利用Redis中原子性的自增操作，我们可以统计类似用户点赞数、用户访问数等，这类操作如果用MySQL，频繁的读写会带来相当大的压力；限速器比较典型的使用场景是限制某个用户访问某个API的频率，常用的有抢购时，防止用户疯狂点击带来不必要的压力；

好友关系，利用集合的一些命令，比如求交集、并集、差集等。可以方便搞定一些共同好友、共同爱好之类的功能；

简单消息队列，除了Redis自身的发布/订阅模式，我们也可以利用List来实现一个队列机制，比如：到货通知、邮件发送之类的需求，不需要高可靠，但是会带来非常大的DB压力，完全可以用List来完成异步解耦；

Session共享，以PHP为例，默认Session是保存在服务器的文件中，如果是集群服务，同一个用户过来可能落在不同机器上，这就会导致用户频繁登陆；采用Redis保存Session后，无论用户落在那台机器上都能够获取到对应的Session信息。
~~~
###简述Redis的数据淘汰机制
 1. volatile-lru  从已设置过期时间的数据集中挑选最近最少使用的数据淘汰
 2. volatile-ttl  从已设置过期时间的数据集中挑选将要过期的数据淘汰
 3. volatile-random 从已设置过期时间的数据集中任意选择数据淘汰
 4. allkeys-lru 从所有数据集中挑选最近最少使用的数据淘汰
 5. allkeys-random 从所有数据集中任意选择数据进行淘汰
 6. noeviction 禁止驱逐数据

Redis怎样防止异常数据不丢失？

RDB 持久化
将某个时间点的所有数据都存放到硬盘上。
可以将快照复制到其它服务器从而创建具有相同数据的服务器副本。
如果系统发生故障，将会丢失最后一次创建快照之后的数据。
如果数据量很大，保存快照的时间会很长。
AOF 持久化
将写命令添加到 AOF 文件（Append Only File）的末尾。
使用 AOF 持久化需要设置同步选项，从而确保写命令同步到磁盘文件上的时机。这是因为对文件进行写入并不会马上将内容同步到磁盘上，而是先存储到缓冲区，然后由操作系统决定什么时候同步到磁盘。有以下同步选项：
选项同步频率always每个写命令都同步everysec每秒同步一次no让操作系统来决定何时同步
always 选项会严重减低服务器的性能；
everysec 选项比较合适，可以保证系统崩溃时只会丢失一秒左右的数据，并且 Redis 每秒执行一次同步对服务器性能几乎没有任何影响；
no 选项并不能给服务器性能带来多大的提升，而且也会增加系统崩溃时数据丢失的数量
随着服务器写请求的增多，AOF 文件会越来越大。Redis 提供了一种将 AOF 重写的特性，能够去除 AOF 文件中的冗余写命令。

###缓存雪崩以及缓存击

> 缓存穿透：就是客户持续向服务器发起对不存在服务器中数据的请求。客户先在Redis中查询，查询不到后去数据库中查询。
缓存击穿：就是一个很热门的数据，突然失效，大量请求到服务器数据库中
缓存雪崩：就是大量数据同一时间失效。


缓存穿透：
1. 接口层增加校验，对传参进行校验，比如说我们的id是从1开始的，那么id<=0的直接拦截；
2. 缓存中取不到的数据，在数据库中也没有取到，这时可以将key-value对写为key-null，这样可以防止攻击用户反复用同一个id暴力攻击
缓存击穿：
最好的办法就是设置热点数据永不过期，拿到刚才的比方里，那就是你买腾讯一个永久会员
缓存雪崩：
1.缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。
2.如果缓存数据库是分布式部署，将热点数据均匀分布在不同得缓存数据库中。

##手写一个LRU算法
思想：使用LinkedHashMap，一个有序的HashMap。
~~~java
import java.util.LinkedHashMap;
import java.util.Map;
 
public class LRUCache<K, V> extends LinkedHashMap<K, V> {
    private final int CACHE_SIZE;
 
    /**
     * 传递进来最多能缓存多少数据
     * @param cacheSize 缓存大小
     */
    public LRUCache(int cacheSize) {
        
        //true 表示让 linkedHashMap 按照访问顺序来进行排序，最近访问的放在头部，最老访问的的放在尾部
        super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true);
        CACHE_SIZE = cacheSize;
    }
 
    @Override
    protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
        //当map中的数据量大于制定的缓存个数的时候，就自动删除最老的数据
        return size() > CACHE_SIZE;
    }
}
~~~
##进程与线程通信方式
###进程间通信
　　管道（pipe）：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用进程间的亲缘关系通常是指父子进程关系。

　　命名管道（named pipe/FIFO）：命名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。

　　信号量（semophonre）：信号量是一个计数器，可以用来控制多个进程队共享资源的访问。它常作为一个锁机制，防止某进程在访问共享资源时，其他进程也访问此资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。

　　消息队列（message queue）：消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。

　　信号（sinal）：信号是一种比较复杂的通信方式，用于通知接受进程某个事件已经发生。

　　共享内存（shared memory）：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的ipc通信方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往和其他通信方式如信号量，配合使用来实现进程间的同步和通信。

　　套接字（socket）：套接字也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同设备间的进程通信。

　　全双工管道：共享内存、信号量、消息队列、管道和命名管道只适用于本地进程间通信，套接字和全双工管道可用于远程通信，因此可用于网络编程。

　　

###线程间通信
　　锁机制：包括互斥锁、条件变量、读写锁

　　　　互斥锁：提供了以排他方式防止数据结构被并发修改的方法。

　　　　读写锁：允许多个线程同时共享数据，而对写操作是互斥的。

　　　　条件变量：可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。

　　信号量机制（Semaphore）：包括无名进程信号量和命名线程信号量

　　信号机制（Signal）：类似进程间的信号处理

## 了解过JVM调优没，基本思路是什么

如果CPU使用率较高，GC频繁且GC时间长，可能就需要JVM调优了。
基本思路就是让每一次GC都回收尽可能多的对象，
对于CMS来说，要合理设置年轻代和年老代的大小。该如何确定它们的大小呢？这是一个迭代的过程，可以先采用JVM的默认值，然后通过压测分析GC日志。

如果看年轻代的内存使用率处在高位，导致频繁的Minor GC，而频繁GC的效率又不高，说明对象没那么快能被回收，这时年轻代可以适当调大一点。

如果看年老代的内存使用率处在高位，导致频繁的Full GC，这样分两种情况：如果每次Full GC后年老代的内存占用率没有下来，可以怀疑是内存泄漏；

如果Full GC后年老代的内存占用率下来了，说明不是内存泄漏，要考虑调大年老代。

对于G1收集器来说，可以适当调大Java堆，因为G1收集器采用了局部区域收集策略，单次垃圾收集的时间可控，可以管理较大的Java堆。

##海量数据的解决方案：
页面上：
使用缓存；页面静态化技术；
数据库层面：
分离数据库中活跃的数据；批量读取和延迟修改；读写分离；使用NoSQL和Hadoop等技术；分布式部署数据库；应用服务和数据服务分离；
其他方面：
使用搜索引擎搜索数据库中的数据；进行业务的拆分；
高并发情况下的解决方案：
应用程序和静态资源文件进行分离，静态资源可以使用CDN；
集群与分布式；
使用Nginx反向代理；

##缓存的实现原理，设计缓存要注意什么
将热点数据放在内存中，用户查询时命中内存中的数据而不用到数据库中查询
注意缓存的一致性，缓存雪崩、击穿、穿透的问题

